# Virtwork Architecture

## Overview

Virtwork is a CLI tool that creates virtual machines on OpenShift clusters (with OpenShift Virtualization / CNV installed) and runs continuous workloads inside them. The goal is to produce realistic CPU, memory, database, network, and disk I/O metrics for monitoring systems (Prometheus, Grafana).

It is a **one-shot deployment tool** — it creates resources and exits. Workload lifecycle management is delegated to systemd inside each VM.

---

## Layered Architecture

The codebase is organized into five dependency layers. Each layer depends only on layers below it.

```mermaid
graph TD
    subgraph "Layer 4 — Orchestration"
        CMD["cmd/virtwork/main.go\nCobra commands + orchestration"]
        CLEANUP["internal/cleanup/cleanup.go\nlabel-based teardown\n(VMs, Services, Secrets)"]
    end

    subgraph "Layer 3 — Workload Definitions"
        REGISTRY["internal/workloads/registry.go\nregistry + lookup"]
        IFACE["internal/workloads/workload.go\nWorkload interface"]
        CPU["internal/workloads/cpu.go\nstress-ng CPU"]
        MEM["internal/workloads/memory.go\nstress-ng VM memory"]
        DB["internal/workloads/database.go\nPostgreSQL + pgbench"]
        NET["internal/workloads/network.go\niperf3 server/client"]
        DISK["internal/workloads/disk.go\nfio profiles"]
    end

    subgraph "Layer 2 — K8s Abstractions"
        VM["internal/vm/vm.go\nVM spec CRUD + retry"]
        RES["internal/resources/resources.go\nnamespace + service + secret"]
        WAIT["internal/wait/wait.go\nVMI readiness polling"]
    end

    subgraph "Layer 1 — Infrastructure"
        CLUSTER["internal/cluster/cluster.go\ncontroller-runtime client init"]
        CONFIG["internal/config/config.go\nViper config"]
        CLOUDINIT["internal/cloudinit/cloudinit.go\ncloud-config YAML builder"]
    end

    subgraph "Layer 0 — Definitions"
        CONST["internal/constants/constants.go\nAPI coords, labels, defaults"]
    end

    CMD --> CONFIG
    CMD --> CLUSTER
    CMD --> REGISTRY
    CMD --> VM
    CMD --> RES
    CMD --> WAIT
    CMD --> CLEANUP

    CLEANUP --> VM
    CLEANUP --> RES
    CLEANUP --> CONST

    REGISTRY --> CPU
    REGISTRY --> MEM
    REGISTRY --> DB
    REGISTRY --> NET
    REGISTRY --> DISK

    CPU --> IFACE
    MEM --> IFACE
    DB --> IFACE
    NET --> IFACE
    DISK --> IFACE

    VM --> CLUSTER
    VM --> CONST
    VM --> CLOUDINIT

    RES --> CLUSTER
    RES --> CONST

    WAIT --> CLUSTER
    WAIT --> CONST

    CLUSTER --> CONST
    CONFIG --> CONST

    MAIN["cmd/virtwork/main.go"] --> CMD
```

---

## Concurrency Model

Go's native concurrency eliminates the need for async/sync bridging. All I/O operations run naturally in goroutines, coordinated by `errgroup` and controlled by `context.Context`.

```mermaid
graph LR
    subgraph "Orchestration (main goroutine)"
        CLI_RUN["Cobra RunE\nentry point"]
        ERRGRP["errgroup.Group\nparallel VM creation\nconcurrent polling"]
    end

    subgraph "Goroutines (spawned by errgroup)"
        G1["goroutine: CreateVM cpu-0"]
        G2["goroutine: CreateVM disk-0"]
        G3["goroutine: CreateVM db-0"]
        G4["goroutine: WaitForVMReady cpu-0"]
    end

    subgraph "Context"
        CTX["context.WithTimeout\ncancellation + deadline"]
    end

    CLI_RUN --> ERRGRP
    ERRGRP --> G1
    ERRGRP --> G2
    ERRGRP --> G3
    ERRGRP --> G4
    CTX --> G1
    CTX --> G2
    CTX --> G3
    CTX --> G4
```

```mermaid
graph LR
    subgraph "controller-runtime client"
        CR["client.Client\ntyped Get/Create/List/Delete"]
    end

    subgraph "K8s API Types"
        KV["kubevirtv1.VirtualMachine"]
        VMI["kubevirtv1.VirtualMachineInstance"]
        CDI["cdiv1beta1.DataVolume"]
        CORE["corev1.Namespace / Service"]
    end

    CR --> KV
    CR --> VMI
    CR --> CDI
    CR --> CORE
```

| Package | Goroutines | Rationale |
|---------|-----------|-----------|
| `internal/constants` | No | Pure values, no I/O |
| `internal/config` | No | One-time Viper load at startup |
| `internal/cloudinit` | No | Pure string/YAML generation |
| `internal/cluster` | No | One-time client init at startup |
| `internal/vm` | Yes | CRUD operations run in errgroup goroutines; retry loops use `time.Sleep` |
| `internal/resources` | Yes | Namespace/Service/Secret creation can run concurrently |
| `internal/wait` | Yes | Concurrent VMI polling via errgroup; uses `time.Sleep` between polls |
| `internal/workloads` | No | Pure data producers (cloud-init specs, resource structs) |
| `internal/cleanup` | No | Sequential VM/Service/Secret deletion with error accumulation |
| `cmd/virtwork` | Yes | Owns errgroup lifecycle; spawns goroutines for parallel operations |

---

## CLI Orchestration Flow

```mermaid
flowchart TD
    START([virtwork run]) --> LOAD_CFG[Load config via Viper\nflags > env > file > defaults]
    LOAD_CFG --> DRY_CHECK{--dry-run?}

    DRY_CHECK -->|Yes| GEN_SPECS[Generate VM specs\nfor each workload]
    GEN_SPECS --> PRINT_YAML[Print specs as YAML]
    PRINT_YAML --> EXIT_D([Exit])

    DRY_CHECK -->|No| CONNECT[Connect to cluster\ncontroller-runtime client.New]
    CONNECT --> ENSURE_NS[EnsureNamespace]
    ENSURE_NS --> CTX_CREATE[Create context.WithTimeout]
    CTX_CREATE --> WORKLOAD_LOOP[For each enabled workload]

    WORKLOAD_LOOP --> GET_WL[Get workload from registry]
    GET_WL --> GEN_CI[Generate cloud-init userdata]
    GEN_CI --> SVC_CHECK{RequiresService?}
    SVC_CHECK -->|Yes| CREATE_SVC[CreateService\nmust exist before VMs for DNS]
    SVC_CHECK -->|No| SPAWN_VMS
    CREATE_SVC --> SPAWN_VMS[Spawn goroutines via errgroup\nBuildVMSpec + CreateVM]
    SPAWN_VMS --> MORE_WL{More workloads?}
    MORE_WL -->|Yes| WORKLOAD_LOOP
    MORE_WL -->|No| ERRGRP_WAIT[errgroup.Wait\nall VM creates]

    ERRGRP_WAIT --> WAIT_CHECK{--no-wait?}
    WAIT_CHECK -->|Yes| PRINT_SUMMARY[Print summary table]
    WAIT_CHECK -->|No| POLL[WaitForAllVMsReady\nerrgroup for concurrent polling]
    POLL --> PRINT_SUMMARY
    PRINT_SUMMARY --> EXIT([Exit])
```

```mermaid
flowchart TD
    START_C([virtwork cleanup]) --> LOAD_CFG_C[Load config via Viper]
    LOAD_CFG_C --> CONNECT_C[Connect to cluster]
    CONNECT_C --> DO_CLEANUP[CleanupAll\ndelete labeled VMs + Services]
    DO_CLEANUP --> PRINT_SUMMARY_C[Print cleanup summary]
    PRINT_SUMMARY_C --> EXIT_C([Exit])
```

---

## Workload Architecture

Each workload implements the `Workload` interface and produces cloud-init userdata and VM resource requirements. Workloads do not perform any I/O — they are pure data producers.

```mermaid
classDiagram
    class Workload {
        <<interface>>
        +Name() string
        +CloudInitUserdata() (string, error)
        +VMResources() VMResourceSpec
        +ExtraVolumes() []Volume
        +ExtraDisks() []Disk
        +DataVolumeTemplates() []DataVolumeTemplateSpec
        +RequiresService() bool
        +ServiceSpec() *Service
        +VMCount() int
    }

    class BaseWorkload {
        +Config WorkloadConfig
        +SSHUser string
        +SSHPassword string
        +SSHAuthorizedKeys []string
        +VMResources() VMResourceSpec
        +ExtraVolumes() []Volume
        +ExtraDisks() []Disk
        +DataVolumeTemplates() []DataVolumeTemplateSpec
        +RequiresService() false
        +ServiceSpec() nil
        +VMCount() int (Config.VMCount or 1)
        +BuildCloudConfig(opts) (string, error)
    }

    class CPUWorkload {
        +Name() "cpu"
        +CloudInitUserdata() stress-ng --cpu config
        +VMResources() cpu/memory from config
    }

    class MemoryWorkload {
        +Name() "memory"
        +CloudInitUserdata() stress-ng --vm config
        +VMResources() cpu/memory from config
    }

    class DatabaseWorkload {
        +Name() "database"
        +CloudInitUserdata() postgresql + pgbench
        +DataVolumeTemplates() blank DV for /var/lib/pgsql/data
    }

    class NetworkWorkload {
        +Namespace string
        +Name() "network"
        +VMCount() count * 2 (server+client pairs)
        +RequiresService() true
        +ServerUserdata() iperf3 -s
        +ClientUserdata() iperf3 -c
        +ServiceSpec() ClusterIP for server
    }

    class DiskWorkload {
        +Name() "disk"
        +CloudInitUserdata() fio profiles
        +DataVolumeTemplates() blank DV for /mnt/data
    }

    Workload <|.. BaseWorkload
    BaseWorkload <|-- CPUWorkload
    BaseWorkload <|-- MemoryWorkload
    BaseWorkload <|-- DatabaseWorkload
    BaseWorkload <|-- NetworkWorkload
    BaseWorkload <|-- DiskWorkload
```

`BaseWorkload` is an embedded struct that provides default implementations for optional interface methods. Concrete workloads embed `BaseWorkload` and override only the methods they need — idiomatic Go composition over inheritance.

`BaseWorkload` also stores SSH credential fields and exposes a `BuildCloudConfig(opts)` helper method that injects SSH user/password/keys into the cloud-init output. Workload subclasses call `w.BuildCloudConfig(opts)` instead of `cloudinit.BuildCloudConfig(opts)` directly, keeping SSH injection as a single cross-cutting concern on the base struct.

### Workload Comparison

| Workload | VM Count | Data Volume | K8s Service | Packages | Workload Tool |
|----------|----------|-------------|-------------|----------|---------------|
| CPU | N (configurable) | No | No | stress-ng | `stress-ng --cpu 0 --cpu-method all` |
| Memory | N (configurable) | No | No | stress-ng | `stress-ng --vm 1 --vm-bytes 80% --vm-method all` |
| Database | N (configurable) | Yes (`/var/lib/pgsql/data`) | No | postgresql-server | `pgbench -c 10 -j 2 -T 300` loop |
| Network | N×2 (server + client pairs) | No | Yes (ClusterIP) | iperf3 | `iperf3 -s` / `iperf3 -c ... --bidir` |
| Disk | N (configurable) | Yes (`/mnt/data`) | No | fio | Mixed R/W + sequential write profiles |

---

## Resource Tracking and Cleanup

All created resources are labeled with `app.kubernetes.io/managed-by: virtwork`. Cleanup queries by label selector — no state file needed. This is resilient to crashes (works even if the tool terminated mid-creation).

```mermaid
flowchart LR
    subgraph "Create"
        VM1["VM: virtwork-cpu-0\nlabels: managed-by=virtwork"]
        VM2["VM: virtwork-disk-0\nlabels: managed-by=virtwork"]
        SVC["Service: virtwork-iperf3-server\nlabels: managed-by=virtwork"]
        SEC["Secret: virtwork-cpu-0-cloudinit\nlabels: managed-by=virtwork"]
        NS["Namespace: virtwork"]
    end

    subgraph "Cleanup Query"
        SEL["client.MatchingLabels\nmanaged-by=virtwork"]
    end

    subgraph "Delete"
        DEL["client.Delete each matched resource\nerrors logged, not fatal"]
    end

    SEL --> VM1
    SEL --> VM2
    SEL --> SVC
    SEL --> SEC
    DEL --> NS
```

---

## SSH Credential Flow

SSH credentials are a cross-cutting concern that flows through every layer:

```mermaid
flowchart LR
    CLI["CLI flags\n--ssh-user, --ssh-password\n--ssh-key, --ssh-key-file"]
    ENV["Env vars\nVIRTWORK_SSH_USER\nVIRTWORK_SSH_PASSWORD\nVIRTWORK_SSH_AUTHORIZED_KEYS"]
    YAML["Config YAML\nssh_user, ssh_password\nssh_authorized_keys"]

    CLI --> CONFIG["Config struct\nSSHUser, SSHPassword\nSSHAuthorizedKeys"]
    ENV --> CONFIG
    YAML --> CONFIG

    CONFIG --> ORCH["Orchestration\npasses SSH opts to registry"]
    ORCH --> BASE["BaseWorkload\nstores SSH fields"]
    BASE --> HELPER["BuildCloudConfig helper\ninjects users block"]
    HELPER --> CI["cloud-init userdata\n#cloud-config with users block"]
    CI --> VM["VM spec\ncloudInitNoCloud.userData"]
```

List fields (`SSHAuthorizedKeys`) require special handling at each config layer: YAML passes lists directly, environment variables use comma separation, and CLI merges values from both `--ssh-key` (inline) and `--ssh-key-file` (file path) flags.

---

## Configuration Priority Chain

```mermaid
flowchart LR
    COBRA["Cobra flags\n--namespace virtwork-test"]
    ENV["Viper env vars\nVIRTWORK_NAMESPACE"]
    YAML["Viper config file\nnamespace: virtwork-prod"]
    DEFAULTS["Viper defaults\nnamespace: virtwork"]

    COBRA -->|highest priority| MERGE
    ENV -->|2nd| MERGE
    YAML -->|3rd| MERGE
    DEFAULTS -->|lowest| MERGE
    MERGE["Merged Config\nstruct"] --> RUN["Runtime"]
```

Viper's built-in priority chain handles this natively when bound to Cobra flags:
1. Cobra flag explicitly set by user
2. Environment variable (`VIRTWORK_` prefix, automatic binding)
3. Config file (YAML, loaded via `viper.ReadInConfig()`)
4. Default value (set via `viper.SetDefault()`)

---

## Key Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Boot disk | `containerDisk` | Fast kubelet image pull, cached on nodes. Ephemeral root is fine for workload VMs. |
| Data disk | Blank `DataVolume` | Formatted on first boot by cloud-init. Only needed for database and fio workloads. |
| Workload lifecycle | systemd services | Survive reboots, auto-restart on failure, proper logging via journald. |
| Network coordination | K8s Service + DNS | No IP polling from Go. Client retries via systemd `Restart=always`. |
| Cleanup tracking | Label selectors | No state file. Works even if tool crashed mid-creation. |
| Auth | In-cluster first, kubeconfig fallback | Works both inside pods (CI/CD) and from developer machines. |
| Concurrency | goroutines + `errgroup` | Native Go concurrency with structured error collection. No async/sync bridge needed. |
| K8s client | controller-runtime `client.Client` | Typed CRUD operations. Scheme-based serialization for KubeVirt/CDI types. Common in OpenShift ecosystem. |
| Idempotency | AlreadyExists = skip | Safe to re-run. Enables declarative approach. |
| Retry | Backoff for rate-limited/5xx | Handles transient cluster issues. NotFound/Unauthorized/Forbidden are fatal (configuration errors). |
| SSH credential injection | `BaseWorkload.BuildCloudConfig()` helper | Cross-cutting concern handled once in base struct. Workloads call one method. |
| Multi-VM orchestration | `MultiVMWorkload` interface + `VMCount() > 1` | Generic detection — future multi-VM workloads work without orchestration changes. |
| Network VM scaling | `VMCount() = count * 2` | Honors `--vm-count` to create N server/client pairs instead of a single hardcoded pair. |
| Cloud-init Secrets | `CloudInitSecretName` → `UserDataSecretRef` | For large userdata, stores cloud-init in a K8s Secret instead of inline in the VM spec. |
| Cleanup error semantics | Sequential per-resource deletion with error accumulation | Different from create-time error handling (which is fail-fast). Cleanup continues on individual failures. |

---

## Project Structure

```
virtwork/
├── cmd/
│   └── virtwork/
│       └── main.go                # Cobra root + subcommands, orchestration
├── internal/
│   ├── constants/
│   │   └── constants.go           # API coords, labels, defaults
│   ├── config/
│   │   └── config.go              # Config struct, Viper priority chain
│   ├── cluster/
│   │   └── cluster.go             # controller-runtime client init + scheme registration
│   ├── cloudinit/
│   │   └── cloudinit.go           # Cloud-config YAML builder
│   ├── vm/
│   │   └── vm.go                  # VM spec construction + typed CRUD + retry
│   ├── resources/
│   │   └── resources.go           # Namespace + Service + Secret helpers
│   ├── wait/
│   │   └── wait.go                # VMI readiness polling (errgroup)
│   ├── cleanup/
│   │   └── cleanup.go             # Label-based teardown (VMs, Services, Secrets)
│   ├── workloads/
│   │   ├── workload.go            # Workload interface + BaseWorkload
│   │   ├── registry.go            # Registry map + lookup
│   │   ├── cpu.go                 # stress-ng CPU continuous workload
│   │   ├── memory.go              # stress-ng VM memory pressure workload
│   │   ├── database.go            # PostgreSQL + pgbench loop
│   │   ├── network.go             # iperf3 server/client pair
│   │   └── disk.go                # fio mixed I/O profiles
│   └── testutil/
│       ├── testutil.go            # Shared test helpers (namespace, connect, cleanup)
│       └── binary.go              # Binary build/run helpers for E2E
├── tests/
│   └── e2e/                       # E2E acceptance tests (//go:build e2e)
├── docs/
│   ├── architecture.md            # This file
│   ├── development.md             # Developer guide
│   ├── implementation-plan.md     # Phased build plan
│   ├── openshift-virtualization-workload-automation.md  # Design plan
│   └── engineering-journals/      # Per-phase development journals
├── go.mod
├── go.sum
└── CLAUDE.md
```
